# -*- coding: utf-8 -*-
"""Imbalanced techniques using ANN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r1zFZMU-iCIgKdTEPtCsuX_f0d7i6C74
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np
# %matplotlib inline

df = pd.read_csv("customer_churn.csv")
df.sample(5)

df.drop('customerID',axis='columns',inplace=True)
df.dtypes

df.TotalCharges.values

df.MonthlyCharges.values

pd.to_numeric(df.TotalCharges,errors='coerce')

pd.to_numeric(df.TotalCharges,errors='coerce').isnull()

df[pd.to_numeric(df.TotalCharges,errors='coerce').isnull()]
# this dataframe will show u the null values in the TotalCharges

df[pd.to_numeric(df.TotalCharges,errors='coerce').isnull()].shape
# so 11 rows are null

df.iloc[488]['TotalCharges'] # as we can see the row 488 is null ' '

df1  = df[df.TotalCharges!=' ']
df1.shape

df1.TotalCharges = pd.to_numeric(df1.TotalCharges)

df1.TotalCharges.dtypes

tenure_churn_no = df1[df1.Churn=='No'].tenure
tenure_churn_yes = df1[df1.Churn=='Yes'].tenure

plt.xlabel("tenure")
plt.ylabel("Number Of Customers")
plt.title("Customer Churn Prediction Visualiztion")

plt.hist([tenure_churn_yes, tenure_churn_no],  color=['green','red'],label=['Churn=Yes','Churn=No'])
plt.legend()

def print_unique_col_values(df):
       for column in df:
            if df[column].dtypes=='object':
                print(f'{column}: {df[column].unique()}')
                # calling this function will be printing unique values for all your categorical columns

print_unique_col_values(df1)

df1.replace('No internet service','No',inplace=True)
df1.replace('No phone service','No',inplace=True)

print_unique_col_values(df1)

yes_no_columns = ['Partner','Dependents','PhoneService','MultipleLines','OnlineSecurity','OnlineBackup',
                  'DeviceProtection','TechSupport','StreamingTV','StreamingMovies','PaperlessBilling','Churn']
#as we know machine learning doesnt understand text best way to convert it to 0 and 1
for col in yes_no_columns:
    df1[col].replace({'Yes': 1,'No': 0},inplace=True)

for col in df1:
    print(f'{col}: {df1[col].unique()}')

df1['gender'].replace({'Female':1,'Male':0},inplace=True)

df1.gender.unique()

df2 = pd.get_dummies(data=df1, columns=['InternetService','Contract','PaymentMethod'])
df2.columns

df2.sample(5)

df2.dtypes

cols_to_scale = ['tenure','MonthlyCharges','TotalCharges']

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()  #converting values into range between 0 and 1
df2[cols_to_scale] = scaler.fit_transform(df2[cols_to_scale])

df2.sample(5) # as we can see the samples are converted into range between 0s and 1s

for col in df2:
    print(f'{col}: {df2[col].unique()}')

# now my data frame is ready to be used

X = df2.drop('Churn',axis='columns')
y = df2['Churn']

from sklearn.model_selection import train_test_split # spliting our dataset into train and test samples
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=5)
# we used 80% as train and 20% as a test samples

X_train.shape

X_test.shape

len(X_train.columns) # 26 columns as a training and 1 column was dropped

import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(20,input_shape =(26,),activation='relu'),
    # keras.layer.Dense(15,activation='relu'), no need for a one more dense layer since we have 1 inpurt layer and there as 1 hidden layer and an output layer
    keras.layers.Dense(1,activation='sigmoid'),

])
model.compile(optimizer='adam', # learning rate of adam is 0.01
              loss='binary_crossentropy', # beacuse our output is binary 0 and 1
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=100)

model.evaluate(X_test,y_test)

yp = model.predict(X_test)
yp[:5]

y_test[:5]

y_pred = []
for element in yp:
    if element > 0.5:
        y_pred.append(1)
    else:
        y_pred.append(0)

y_pred[:5]

from sklearn.metrics import confusion_matrix , classification_report

print(classification_report(y_test,y_pred))

import seaborn as sn
cm = tf.math.confusion_matrix(labels=y_test,predictions=y_pred)

plt.figure(figsize = (10,7))
sn.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')

#879 + 223 our model got it correct
#185 + 120 our model got an error

round((879+223)/(879+223+185+120),2) # Accuracy is 0.78

round(879/(879+223),2) #Precision for 0 class

round(223/(223+120),2) #precison for 1 class

round(879/(879+120),2) # recall for 0 class

round(223/(223+185),2) # recall for 1 class

!pip install tensorflow-addons

from tensorflow_addons import losses

import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix , classification_report

def ANN(X_train, y_train, X_test, y_test, loss, weights):
    model = keras.Sequential([
        keras.layers.Dense(26, input_dim=26, activation='relu'),
        keras.layers.Dense(15, activation='relu'),
        keras.layers.Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])

    if weights == -1:
        model.fit(X_train, y_train, epochs=100)
    else:
        model.fit(X_train, y_train, epochs=100, class_weight = weights)

    print(model.evaluate(X_test, y_test))

    y_preds = model.predict(X_test)
    y_preds = np.round(y_preds)

    print("Classification Report: \n", classification_report(y_test, y_preds))

    return y_preds

y_preds = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)

#Method 1: Undersampling

# Class count
count_class_0, count_class_1 = df1.Churn.value_counts()

# Divide by class
df_class_0 = df2[df2['Churn'] == 0]
df_class_1 = df2[df2['Churn'] == 1]

df_class_0.shape #we can see the imbalanced this class has 5163

df_class_1.shape # and this class has 1869

#now we will undersample the dataset
df_class_0_under = df_class_0.sample(count_class_1)
df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0) #concat the 2 classes with pandas
#df_test_under.shape -->  1869 + 1869 = 3738

print('Random under-sampling:')
print(df_test_under.Churn.value_counts())

count_class_0, count_class_1

X = df_test_under.drop('Churn',axis='columns')
y = df_test_under['Churn']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y) # startify makes sure you have balanced samples

#Number of classes in training Data
y_train.value_counts()

y_preds = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)

#classification result we can see the recall and precision is fair

#Method2: Oversampling

count_class_0,count_class_1

# Oversample 1-class and concat the DataFrames of both classes
df_class_1_over = df_class_1.sample(count_class_0, replace=True)
df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)
df_test_over.shape #5163 * 2 = 10326

print('Random over-sampling:')
print(df_test_over.Churn.value_counts())

X = df_test_over.drop('Churn',axis='columns')
y = df_test_over['Churn']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)

# Number of classes in training Data
y_test.value_counts()

y_train.value_counts()

y_preds = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)

#compared to the original class we can see the improvment in precision , recall,f1-score after applying a fair treatment

#Method3: SMOTE

X = df2.drop('Churn',axis='columns')
y = df2['Churn']

pip install imbalanced-learn

y.value_counts() # as we can see the data is imbalanced

from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy='minority')
X_sm, y_sm = smote.fit_resample(X, y)

y_sm.value_counts()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=15, stratify=y_sm)

y_train.value_counts()

y_preds = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)

#Method4: Use of Ensemble with undersampling

df2.Churn.value_counts()

# Regain Original features and labels
X = df2.drop('Churn',axis='columns')
y = df2['Churn']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)

y_train.value_counts()
#model1 --> class1(1495) + class0(0, 1495)

#model2 --> class1(1495) + class0(1496, 2990)

#model3 --> class1(1495) + class0(2990, 4130)

df3 = X_train.copy()
df3['Churn'] = y_train

df3_class0 = df3[df3.Churn==0]
df3_class1 = df3[df3.Churn==1]

df3_class0.shape , df3_class1.shape

def get_train_batch(df_majority, df_minority, start, end):
    df_train = pd.concat([df_majority[start:end], df_minority], axis=0)

    X_train = df_train.drop('Churn', axis='columns')
    y_train = df_train.Churn
    return X_train, y_train

X_train, y_train = get_train_batch(df3_class0, df3_class1, 0, 1495)

X_train.shape

X_train, y_train = get_train_batch(df3_class0, df3_class1, 0, 1495)

y_pred1 = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)

X_train, y_train = get_train_batch(df3_class0, df3_class1, 1495, 2990)

y_pred2 = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)

X_train, y_train = get_train_batch(df3_class0, df3_class1, 2990, 4130)

y_pred3 = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)

len(y_pred1)

len(y_pred2)

len(y_pred3)

y_pred_final = y_pred1.copy()
for i in range(len(y_pred1)):
    n_ones = y_pred1[i] + y_pred2[i] + y_pred3[i]
    if n_ones>1:
        y_pred_final[i] = 1
    else:
        y_pred_final[i] = 0

cl_rep = classification_report(y_test, y_pred_final)
print(cl_rep)

