# -*- coding: utf-8 -*-
"""Lost or cost function.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ebEKXjNE0wZ1P7biezs4RgPL3Lo9DOjX
"""

# In machine learning and deep learning using "Squared error" is useful
#Going through the samples in ecamulated way is called "Epoch"
# 4 ways of finding the error in ML and DL : Mean absolute error , Mean squared error ,Total error (indivisual error is called "Loss"),Binary cross entopy
import numpy as np

y_predicted = np.array([1,1,0,0,1])
y_true = np.array([0.30,0.7,1,0,0.5])

#Running a for loop array on two arrays in a parallel way "zip" function
#MAE without numpy
def mae(y_predicted,y_true):
  total_error = 0
  for yt , yp in zip(y_true,y_predicted):
      total_error += abs(yt - yp)
  print("Total error:",total_error)
  mae = total_error / len(y_true)
  print("MAE:",mae)
  return mae

mae(y_true,y_predicted)

# power of numpy in one function
np.mean(np.abs(y_predicted-y_true))

np.log([0.00000000000001])

eplison = 1e-15

#value near 0 cuz log(0) is undifined
y_predicted_new = [max(i,eplison) for i in y_predicted]
y_predicted_new

#value near 1
y_predicted_new = [min(i,1-eplison) for i in y_predicted_new]
y_predicted_new

y_predicted_new = np.array(y_predicted_new)
np.log(y_predicted_new)

np.log(y_predicted)

-np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))

