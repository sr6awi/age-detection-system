# -*- coding: utf-8 -*-
"""Batch gradient descent plain python implmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uIA1d_S6vymQagFyK0qk6ihX_q7_ee6Q
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
# %matplotlib inline

df = pd.read_csv("homeprices_banglore.csv")
df.sample(5)

from sklearn import preprocessing

sx = preprocessing.MinMaxScaler()
sy = preprocessing.MinMaxScaler()

scaled_X = sx.fit_transform(df.drop('price',axis = 'columns'))
scaled_X
# scaling makes the values betweeen 0 and 1

scaled_y = sy.fit_transform(df['price'].values.reshape(df.shape[0],1))
scaled_y

scaled_X

a = np.array([5,9,8])
b = np.array([20,15,6])
a-b

np.mean(np.square(a-b))

def batch_gradient_descent(X, y_true, epochs, learning_rate = 0.01):

    number_of_features = X.shape[1]

    w = np.ones(shape=(number_of_features))
    b = 0
    total_samples = X.shape[0] # number of rows in X

    cost_list = []
    epoch_list = []

    for i in range(epochs):
        y_predicted = np.dot(w, X.T) + b

        w_grad = -(2/total_samples)*(X.T.dot(y_true-y_predicted))
        b_grad = -(2/total_samples)*np.sum(y_true-y_predicted)

        w = w - learning_rate * w_grad
        b = b - learning_rate * b_grad

        cost = np.mean(np.square(y_true-y_predicted)) # MSE

        if i%10==0:
            cost_list.append(cost)
            epoch_list.append(i)

    return w, b, cost, cost_list, epoch_list

w, b, cost, cost_list, epoch_list = batch_gradient_descent(scaled_X,scaled_y.reshape(scaled_y.shape[0],),500)
w, b, cost

plt.xlabel("epoch")
plt.ylabel("cost")
plt.plot(epoch_list,cost_list)

# batch gradient descent we go through all the trainning samples and calculates the cumlative error
# then we can back propgate and adjust weights
# note: Good for small data training samples