# -*- coding: utf-8 -*-
"""Flowers classificatin using data augmentation technique.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12cTzhYY-jLNzWjtQBpJdpDxiKNbGTrNt
"""

import matplotlib.pyplot as plt
import numpy as np
import cv2
import os
import PIL #image libary in python
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url,  cache_dir='.', untar=True) #untar is unzipping

data_dir

# Commented out IPython magic to ensure Python compatibility.
# %ls

#converting this data directory into  a pathlib
import pathlib
data_dir = pathlib.Path(data_dir)
data_dir

img_count = len(list(data_dir.glob('*/*.jpg')))
img_count #3670 is my total number of images (dataset)

roses_count = len(list(data_dir.glob('roses/*')))
roses_count

roses = list(data_dir.glob('roses/*'))
roses

PIL.Image.open(str(roses[1]))

tulips = list(data_dir.glob('tulips/*'))
PIL.Image.open(str(tulips[0]))

sunflowers = list (data_dir.glob('sunflowers/*'))
PIL.Image.open(str(sunflowers[0]))

# doing a dictionary of different kinds of flowers and the image path associated which will help me during the model build
flowers_image_dict = {
    'roses' : list(data_dir.glob('roses/*')),
    'daisy' : list(data_dir.glob('daisy/*')),
    'dandelion' : list(data_dir.glob('dandelion/*')),
    'sunflowers' : list(data_dir.glob('sunflowers/*')),
    'tulips' : list(data_dir.glob('tulips/*')),
}

len(flowers_image_dict['sunflowers'])

len(flowers_image_dict['tulips'])

#creating a labes for each kind of flower into a number
flowers_labels_dict = {
    'roses' : 0,
    'daisy' : 1,
    'dandelion' : 2,
    'sunflowers' : 3,
    'tulips' : 4,
}

flowers_labels_dict['roses']

flowers_image_dict['roses'][0]

img = cv2.imread(str(flowers_image_dict['roses'][0])) #cv2 doesnt accept a path as an argument so we will convert into a string array
img.shape
# as we can see our data is in a 3D array (x,y,rgb channel)

#open cv has a function allows us to resize an image
cv2.resize(img,(180,180)).shape
# now our imgs are resized into (180,180,3)

X, y = [], []

for flower_name, images in flowers_image_dict.items():
    for image in images:
        img = cv2.imread(str(image))
        resized_img = cv2.resize(img,(180,180)) #machine learning expects that all images are in the same dimension thats why we are resizing
        X.append(resized_img)
        y.append(flowers_labels_dict[flower_name])

y[:5]

X = np.array(X)
y = np.array(y)

X[0]

#Train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

len(X_train)

len(X_test)

#scaling the images into a range between 0 and 1 which helps in the model training
X_trained_scaled = X_train/255
X_test_scaled = X_test/255

X_train[0]

X_trained_scaled[0]

num_classes = 5
model = Sequential([
  #CNN
  layers.Conv2D(16, 3, padding='same', activation='relu'),#16 filters and 3x3 size
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),

  #flatten
  layers.Flatten(),

  #dense
  layers.Dense(128, activation= 'relu'),

  #last layer(outputlayer) with 5 neurons
  layers.Dense(num_classes)
])

# compile

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])

#training the model

model.fit(X_trained_scaled, y_train , epochs=30)

model.evaluate(X_test_scaled,y_test)

#since we have 99% accuracy of training dataset and 66% of test dataset this is an example of overfiting
predictions = model.predict(X_test_scaled)
predictions

score = tf.nn.softmax(predictions[0])
score

np.argmax([0,780,123,8])

np.argmax(score)

y_test[2]

#data augmentation

data_augmentation = keras.Sequential(
    [
    layers.experimental.preprocessing.RandomContrast(0.3)

    ]
    )

plt.axis('off')
plt.imshow(X[0])

plt.axis('off')
plt.imshow(data_augmentation(X)[0].numpy().astype("uint8"))